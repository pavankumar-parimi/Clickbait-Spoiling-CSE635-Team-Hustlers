{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTgSBK3v1Ls-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets \n",
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prkNHuuN1eYc"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset, load_metric\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from datasets.load import DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "import collections\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import default_data_collator\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbYsLCRm1ies"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Palak/microsoft_deberta-large_squad\"\n",
        "batch_size = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVVpTe2i57s2"
      },
      "outputs": [],
      "source": [
        "class DatasetFormatter:\n",
        "  def __init__(self, test_df_arg) -> None:\n",
        "    self.train_df = pd.read_json(\"train.jsonl\", lines = True)\n",
        "    self.test_df = pd.read_json(\"validation.jsonl\", lines = True)\n",
        "\n",
        "  def list_to_string(self, spoiler_type):\n",
        "    if spoiler_type[0] == \"phrase\":\n",
        "      return 0\n",
        "    elif spoiler_type[0] == \"passage\":\n",
        "      return 1\n",
        "    elif spoiler_type[0] == \"multi\":\n",
        "      return 2\n",
        "\n",
        "  def return_text(self, text):\n",
        "    return text[0]\n",
        "\n",
        "  def preprocess_data(self, data):\n",
        "\n",
        "    no_of_rows = data.shape[0]\n",
        "    formatted_data = []\n",
        "    for index in range(no_of_rows):\n",
        "      complete_description = \" \".join(data.iloc[index].to_dict()[\"targetParagraphs\"])\n",
        "      row = {}\n",
        "      row[\"id\"] = data.iloc[index].to_dict()[\"uuid\"]\n",
        "      row[\"context\"] = complete_description,\n",
        "      row[\"question\"] = data.iloc[index].to_dict()[\"postText\"][0],\n",
        "      row[\"answers\"] = {\n",
        "          \"text\": data.iloc[index].to_dict()[\"spoiler\"],\n",
        "          \"answer_start\": [complete_description.find(data.iloc[index].to_dict()[\"spoiler\"][0])]\n",
        "      }\n",
        "      formatted_data.append(row)\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "  def get_formatted_dataset(self):\n",
        "    train_df = self.train_df\n",
        "    test_df = self.test_df\n",
        "\n",
        "    train_df[\"tags\"] = train_df[\"tags\"].apply(self.list_to_string)\n",
        "    test_df[\"tags\"] = test_df[\"tags\"].apply(self.list_to_string)\n",
        "\n",
        "    # Taking only Pharse dataset\n",
        "    train_df = train_df[train_df['tags']==0]\n",
        "    test_df = test_df[test_df['tags']==0]\n",
        "\n",
        "    train_df = dataset_formatte_obj.preprocess_data(train_df)\n",
        "    test_df = dataset_formatte_obj.preprocess_data(test_df)\n",
        "\n",
        "    train_df= pd.DataFrame(train_df)\n",
        "    test_df = pd.DataFrame(test_df)\n",
        "\n",
        "    # Removing the list\n",
        "    train_df[\"context\"] = train_df[\"context\"].apply(self.return_text)\n",
        "    test_df[\"context\"] = test_df[\"context\"].apply(self.return_text)\n",
        "\n",
        "    # Removing the list\n",
        "    train_df[\"question\"] = train_df[\"question\"].apply(self.return_text)\n",
        "    test_df[\"question\"] = test_df[\"question\"].apply(self.return_text)\n",
        "\n",
        "    # Spliting the train and validation set from training dataset\n",
        "    validation_df = train_df.iloc[1258:]\n",
        "    train_df = train_df.iloc[:1174]\n",
        "\n",
        "    dataset_train = Dataset.from_pandas(train_df)\n",
        "    dataset_validation = Dataset.from_pandas(validation_df)\n",
        "    dataset_test =  Dataset.from_pandas(test_df)\n",
        "\n",
        "    datasets = DatasetDict()\n",
        "\n",
        "    datasets[\"train\"] = dataset_train\n",
        "    datasets[\"validation\"] = dataset_validation\n",
        "    datasets[\"test\"] = dataset_test\n",
        "\n",
        "    return datasets\n",
        "\n",
        "dataset_formatte_obj = DatasetFormatter()\n",
        "\n",
        "datasets = dataset_formatte_obj.get_formatted_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IJBiRkaEicWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkROXbkrrbF2"
      },
      "source": [
        "### Preprocessing the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPhifqdNragF"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "  def __init__(self, model_checkpoint, max_length, doc_stride) -> None:\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    # The maximum length of a feature (question and context)\n",
        "    self.max_length = max_length\n",
        "    self.doc_stride = doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "  def prepare_train_features(self, examples):\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=self.max_length,\n",
        "        stride=self.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    \n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        \n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "\n",
        "        \n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                start_positions.append(cls_index)\n",
        "                end_positions.append(cls_index)\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                start_positions.append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "proprocessor_obj = Preprocessor(model_checkpoint, max_length, doc_stride)\n",
        "tokenized_datasets = datasets.map(proprocessor_obj.prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUizkvJ41jvz"
      },
      "outputs": [],
      "source": [
        "class FineTune:\n",
        "  def __init__(self, model_checkpoint) -> None:\n",
        "    self.model_checkpoint = model_checkpoint\n",
        "    self.model = AutoModelForQuestionAnswering.from_pretrained(self.model_checkpoint)\n",
        "    self.setup()\n",
        "    self.trainer = None\n",
        "\n",
        "  def setup(self):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.model.to(device)\n",
        "    print(f'Working on {device}')\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU Cache removed\")\n",
        "\n",
        "  def trainer_model(self):\n",
        "    model_name = self.model_checkpoint.split(\"/\")[-1]\n",
        "    args = TrainingArguments(\n",
        "        f\"{model_name}-finetuned-webis\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=0.1,\n",
        "        weight_decay=0.01,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "    trainer = Trainer(\n",
        "    self.model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=proprocessor_obj.tokenizer,\n",
        "    )\n",
        "    self.trainer = trainer\n",
        "\n",
        "  def get_trainer(self):\n",
        "    return self.trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WsvOqcZNKHF"
      },
      "outputs": [],
      "source": [
        "fine_tune_obj = FineTune(model_checkpoint)\n",
        "fine_tune_obj.trainer_model()\n",
        "trainer = fine_tune_obj.get_trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbtug2xZ2bh9"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp3Jg5g_2daP"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs5qOxd71jtv"
      },
      "outputs": [],
      "source": [
        "test_datasets = DatasetDict()\n",
        "test_datasets[\"test\"] = datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9-1qqHX1jrb"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pri5aU511jnR"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Evaluate:\n",
        "  def __init__(self, proprocessor_obj):\n",
        "    self.tokenizer = proprocessor_obj.tokenizer\n",
        "    self.max_length = proprocessor_obj.max_length\n",
        "    self.doc_stride = proprocessor_obj.doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "\n",
        "  def prepare_validation_features(self, examples):\n",
        "      \n",
        "      examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "     \n",
        "      tokenized_examples = proprocessor_obj.tokenizer(\n",
        "          examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "          examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "          truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "          max_length=max_length,\n",
        "          stride=doc_stride,\n",
        "          return_overflowing_tokens=True,\n",
        "          return_offsets_mapping=True,\n",
        "          padding=\"max_length\",\n",
        "      )\n",
        "\n",
        "      \n",
        "      sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "      tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "      for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "          sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "          context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "          sample_index = sample_mapping[i]\n",
        "          tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "          tokenized_examples[\"offset_mapping\"][i] = [\n",
        "              (o if sequence_ids[k] == context_index else None)\n",
        "              for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "          ]\n",
        "\n",
        "      return tokenized_examples\n",
        "  \n",
        "  def prepare_test_features(self, examples):\n",
        "    \n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "   \n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "   \n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        \n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "  def postprocess_qa_predictions(self, examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)  \n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            \n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(self.tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    \n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "evaluate_obj = Evaluate(proprocessor_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2l_DGIDzrmb"
      },
      "outputs": [],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    evaluate_obj.prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(validation_features)\n",
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l75VT75f1jaO"
      },
      "outputs": [],
      "source": [
        "test_features = datasets[\"test\"].map(\n",
        "    evaluate_obj.prepare_test_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"test\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(test_features)\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBjRX4dD1rRn"
      },
      "outputs": [],
      "source": [
        "final_predictions = evaluate_obj.postprocess_qa_predictions(test_datasets[\"test\"], test_features, raw_predictions.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcKc2EZo1s7l"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_e_Jqvv14Aw"
      },
      "outputs": [],
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_datasets[\"test\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQlevyg84mMw"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "class EvaluationMetric:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.bleu = evaluate.load('bleu')\n",
        "    self.meteor = evaluate.load('meteor')\n",
        "    self.bertscore = load(\"bertscore\")\n",
        "\n",
        "  def return_target_paragraph(self, id, prediction_text):\n",
        "    test_df = pd.read_json(\"validation.jsonl\", lines = True)\n",
        "    test_df[\"tags\"] = test_df[\"tags\"].apply(self.list_to_string)\n",
        "    test_df = test_df[test_df[\"tags\"] == 1]\n",
        "    target_paragraphs = self.test_target_paragraphs(test_df)\n",
        "\n",
        "    for each in target_paragraphs:\n",
        "      if each[0] == id:\n",
        "        for sentence in each[1]:\n",
        "          if prediction_text in sentence:\n",
        "            return sentence\n",
        "\n",
        "    return prediction_text \n",
        "\n",
        "  def test_target_paragraphs(self, data):\n",
        "    target_paragraphs = []\n",
        "\n",
        "    for row in data.values.tolist():\n",
        "      target_paragraphs.append((row[0],row[3]))\n",
        "    \n",
        "    return target_paragraphs\n",
        "\n",
        "  def list_to_string(self, spoiler_type):\n",
        "    if spoiler_type[0] == \"phrase\":\n",
        "      return 0\n",
        "    elif spoiler_type[0] == \"passage\":\n",
        "      return 1\n",
        "    elif spoiler_type[0] == \"multi\":\n",
        "      return 2\n",
        "\n",
        "  def get_bleu_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    predictions_test = []\n",
        "    references_test = []\n",
        "\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          post_prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"].strip())\n",
        "\n",
        "          predictions_test.append(post_prediction)\n",
        "          references_test.append([ref[\"answers\"][\"text\"][0].strip()])\n",
        "\n",
        "    for i in range(10):\n",
        "      print(\"Prediction comparision: \", i)\n",
        "      print(\"Model prediction --> \", predictions_test[i])\n",
        "      print(\"Reference --> \", references_test[i])\n",
        "      print('\\n')\n",
        "\n",
        "    result = self.bleu.compute(predictions = predictions_test, references = references_test).get(\"bleu\")\n",
        "\n",
        "    # bleu_score = results/count\n",
        "    # return bleu_score\n",
        "    return result\n",
        "\n",
        "  def get_meteor_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    predictions_test = []\n",
        "    references_test = []\n",
        "\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          post_prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"].strip())\n",
        "          predictions_test.append(post_prediction)\n",
        "\n",
        "          # predictions_test.append(each[\"prediction_text\"].strip())\n",
        "          references_test.append(ref[\"answers\"][\"text\"][0].strip())\n",
        "    \n",
        "    result = self.meteor.compute(predictions = predictions_test, references = references_test).get(\"meteor\")\n",
        "\n",
        "    # meteor_score = results/count\n",
        "    # return meteor_score\n",
        "    return result\n",
        "  \n",
        "  def get_bert_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    predictions_test = []\n",
        "    references_test = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          post_prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"].strip())\n",
        "          predictions_test.append(post_prediction)\n",
        "\n",
        "          # predictions_test.append(each[\"prediction_text\"].strip())\n",
        "          references_test.append(ref[\"answers\"][\"text\"][0].strip())\n",
        "    \n",
        "    results = bertscore.compute(predictions=predictions_test, references=references_test, lang = \"en\")\n",
        "\n",
        "    \n",
        "    return results\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyMDjZjyUff0"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "evaluation_metric_obj = EvaluationMetric()\n",
        "bleu_score = evaluation_metric_obj.get_bleu_score(formatted_predictions, references)\n",
        "meteor_score = evaluation_metric_obj.get_meteor_score(formatted_predictions, references)\n",
        "bert_score = evaluation_metric_obj.get_bert_score(formatted_predictions, references)\n",
        "\n",
        "print(\"BLEU Score: \", bleu_score)\n",
        "print(\"METEOR Score: \", meteor_score)\n",
        "print(\"BERT Score: \", mean(bert_score.get(\"f1\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLqjv8aVSPjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLAbyWXanfkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}